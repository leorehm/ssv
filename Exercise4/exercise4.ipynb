{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import get_window, stft\n",
    "from scipy.linalg import solve_toeplitz\n",
    "from util import my_windowing, compute_stft, plot_log_magnitude\n",
    "from filteradaptively import filter_adaptively\n",
    "\n",
    "\n",
    "%matplotlib widget\n",
    "plt.close(\"all\")\n",
    "\n",
    "file = \"Audio/female8khz.wav\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, sr = librosa.load(file, sr=None)\n",
    "print(f\"Sampling Rate: {sr} Hz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.1 Segmentation**\n",
    "\n",
    "Before we start to analyze the speech signal, we first split the signal into overlapping segments with a length of N samples. Each segment overlaps with the previous frame by $L = N−R$ samples, like depicted in Fig. 1. Use a segment length of 32 ms and a segment shift of 8 ms. You may use the function `m_frames, v_time_frame = my_windowing(x, fs, N, R)` from the first exercise session.\n",
    "\n",
    "*Why do we segment the signal prior to analysis instead of processing the whole signal at once?*\n",
    "\n",
    "Time-Frequency Analysis:\n",
    "\n",
    "- Techniques like STFT require dividing the signal into frames.\n",
    "- Allows analysis of frequency content changes over time.\n",
    "- Crucial for understanding and processing speech signals.\n",
    "\n",
    "Dynamic Range:\n",
    "\n",
    "- Speech signals have varying dynamic ranges and temporal variations.\n",
    "- Segmenting isolates these variations for separate analysis.\n",
    "- Leads to better feature extraction and enhancement.\n",
    "\n",
    "*Is a segment length of 32 ms appropriate? Why or why not?*\n",
    "\n",
    " A 32 ms window provides a good balance between time resolution and frequency resolution. Shorter windows offer better time resolution but poorer frequency resolution, while longer windows offer better frequency resolution but poorer time resolution. A 32 ms window is a common compromise used in speech processing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_length = 32\n",
    "frame_shift = 8\n",
    "m_frames, v_time_frame = my_windowing(y, sr, 32, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.2 Signal power**\n",
    "\n",
    "In the following, we want to estimate a set of parameters for each of the segments. We begin with creating a function named compute_power that takes a signal segment as an input variable and returns the signal power,\n",
    "\n",
    "where n is the sample index and x(n) corresponds to the current signal segment. Compute the signal power for each segment and store it in a vector. Display the standard deviation (√σ2x = σx) together with the waveform of the speech signal in one plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_power(seg: np.ndarray) -> float:\n",
    "    return (1/len(seg)) * np.sum(seg**2)\n",
    "\n",
    "compute_power(m_frames[20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.3 Voiced / unvoiced decision**\n",
    "\n",
    "Write another function, `is_voiced`, to also analyze if a segment is voiced or not. Here, we use a simple approach, which is counting the number of zero crossings within in one segment.\n",
    "\n",
    "Definition:  \n",
    "> The zero-crossing rate (ZCR) is the rate at which a signal changes from positive to zero to negative or from negative to zero to positive.\n",
    "\n",
    "- Count the number of zero crossings in each segment and normalize them by the segment length $N$. Apply a threshold to decide if the segment is voiced or not. Try to find an appropriate value for the threshold that allows for a reliable classification. Do not expect it to be 100% perfect, but try your best.\n",
    "- `is_voiced` should return 1 for voiced segments and 0 for unvoiced segments\n",
    "\n",
    "Explanation:\n",
    "- We use np.diff(np.sign(segment)) to find the points where the sign of the segment changes, indicating a zero crossing. np.sum(np.abs(...) > 0) counts these crossings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_voiced(seg: np.ndarray, threshold: float = 0.3) -> int:\n",
    "    zero_crossings = np.sum(np.abs(np.diff(np.sign(seg))) > 0)\n",
    "    zero_crossing_rate = zero_crossings / len(seg)\n",
    "    return 1 if zero_crossing_rate < threshold else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = np.arange(0, len(y)) / sr\n",
    "\n",
    "v_uv_frames = np.asarray([is_voiced(x, 0.3) for x in m_frames])\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "fig.set_figheight(4)\n",
    "fig.set_figwidth(10)\n",
    "\n",
    "ax1.set_ylabel(\"time [s]\")\n",
    "ax1.set_xlabel(\"Amplitude\")\n",
    "\n",
    "ax1.plot(time, y)\n",
    "ax1.plot(v_time_frame, v_uv_frames*0.1)\n",
    "\n",
    "plot_log_magnitude(*compute_stft(y, sr, frame_length, frame_shift, get_window(\"hann\", int(sr * frame_length / 1000), True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Why might the number of zero crossings provide valuable information for the voiced/unvoiced decision?*\n",
    "\n",
    "Voiced Sounds:\n",
    "- Have a more regular, periodic waveform with fewer zero crossings.\n",
    "\n",
    "Unvoiced Sounds:\n",
    "- Have a more irregular, noisy waveform with more zero crossings.\n",
    "\n",
    "Zero Crossings:\n",
    "- Counting zero crossings helps distinguish between the regularity of voiced sounds and the irregularity of unvoiced sounds.\n",
    "\n",
    "*In general: are all speech sounds either voiced or unvoiced? Can you think of other speech sounds? Examples?*\n",
    "- Whispered Speech\n",
    "- Clicks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.4 Fundamental frequency estimation**\n",
    "\n",
    "In voiced sounds, the vocal cords periodically open and close. The frequency at which they open and close is called the fundamental frequency, $f_0$.\n",
    "\n",
    "You have already implemented a method to estimate the fundamental frequency based on the autocorrelation function (ACF) in the first exercise session. We can now reuse it here. Put your estimator into a Python function of the form `def estimate_f0(m_frames: np.ndarray, fs: int) -> [np.ndarray (v_f0_estimates)]` where `v_f0_estimates` is a vector containing the fundamental frequency estimates in [Hz] for each signal segment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_f0(m_frames: np.ndarray, fs: int) -> np.ndarray:\n",
    "    fundamental_freqs = []\n",
    "    \n",
    "    # Step b: Compute ACF for each frame and step d: Estimate fundamental frequency\n",
    "    for frame in m_frames:\n",
    "        # ACF via convolution\n",
    "        acf = np.convolve(frame, frame[::-1], mode='full')\n",
    "        \n",
    "        # Step c: Focus on non-negative lags\n",
    "        acf = acf[len(acf)//2:]\n",
    "        \n",
    "        # Convert frequency range to corresponding lag range\n",
    "        min_lag = fs // 400  # Maximum frequency\n",
    "        max_lag = fs // 80   # Minimum frequency\n",
    "                \n",
    "        # Step d: Identify peak within the lag range corresponding to 80 Hz to 400 Hz\n",
    "        peak_lag = np.argmax(acf[min_lag:max_lag]) + min_lag\n",
    "        fundamental_freq = fs / peak_lag\n",
    "        \n",
    "        fundamental_freqs.append(fundamental_freq)\n",
    "    \n",
    "    return np.asarray(fundamental_freqs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot your fundamental frequency estimate together with the spectrogram of the clean speech. For this, the y-axis needs to be in Hz. Use the function `compute_stft` from exercise 2 to compute the spectrogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_stft, v_freq, v_time = compute_stft(y, sr, frame_length, frame_shift, get_window(\"hann\", int(sr * frame_length / 1000), True))\n",
    "v_f0 = estimate_f0(m_frames, sr)\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.set_figwidth(12)\n",
    "ax = fig.add_subplot(111)\n",
    "ax.set_ylabel(\"frequency [Hz]\")\n",
    "ax.set_xlabel(\"time [s]\")\n",
    "im = ax.imshow(\n",
    "    10 * np.log10(np.maximum(np.square(np.abs(m_stft.T)), 10 ** (-15))),\n",
    "    cmap=\"viridis\",\n",
    "    origin=\"lower\",\n",
    "    extent=[v_time[0], v_time[-1], v_freq[0], v_freq[-1]],\n",
    "    aspect=\"auto\",\n",
    ")\n",
    "ax.plot(v_time, v_f0, \"r\", linewidth=0.8)\n",
    "colorbar = fig.colorbar(im, orientation=\"vertical\", pad=0.05)\n",
    "colorbar.set_label(\"dB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.5 Linear prediction coefficients / linear predictive coding**\n",
    "\n",
    "In the lecture, you learned that the form of the vocal tract is crucial to speech production and that it can be modeled by concatenating sections of lossless tubes. This model can well be described in terms of reflection coefficients for each section border. A closely related description can be obtained in terms of an all-pole filter, i.e. its filter coefficients, known as the linear prediction coefficients (LPCs). A common, since very efficient, method to compute the LPCs is the so-called Levinson-Durbin algorithm. Although not necessary at this point, it is worth mentioning that LPCs can be transformed into reflection coefficients and vice versa. This will come in handy in Section 4.\n",
    "- Shortly outline the Source-Filter-Model for speech production.\n",
    "- Compute the LPCs of the signal segments based on the code you have from exercise session three. For that, write a Python function `def compute_lpc(m_frames: np.ndarray, M: int)->[np.ndarray (m_lpc)]` that returns the LP coefficients of each signal segment in the matrix `m_lpc`, which is of size `[num_frames x M]`.\n",
    "- Choose a suitable model order $M$ for linear prediction for a signal with an audio bandwidth of 4 kHz. Give reasons for your choice.\n",
    "\n",
    "Model order $M=12$, because TODO\n",
    "\n",
    "Now we have all parameters we need to decently describe our speech signal. In the next section, we will try to reconstruct the actual waveform of the speech signal based only on this parameter set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coefficients(signal: np.ndarray, lpc_order: int = 12) -> np.ndarray:\n",
    "    # compute the autocorrelation vector\n",
    "    phi_full = np.correlate(signal, signal, mode=\"full\")  # mode \"valid\" | \"full\"\n",
    "\n",
    "    # autocorrelation is symmetric, we only need one half\n",
    "    center = len(phi_full) // 2\n",
    "    phi = phi_full[center:center + lpc_order + 1]\n",
    "\n",
    "    # The right-hand side vector is the remaining part\n",
    "    b = phi[1:]\n",
    "\n",
    "    # Solve for the LPC coefficients\n",
    "    a = solve_toeplitz((phi[:-1], phi[:-1]), b)\n",
    "\n",
    "    return -a\n",
    "\n",
    "def compute_lpc(m_frames: np.ndarray, M: int = 12) -> np.ndarray:\n",
    "    return np.asarray([get_coefficients(frame, M) for frame in m_frames])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3 LPC-vocoder: synthesis**\n",
    "\n",
    "In this section, lets assume that the speech signal is recorded and analyzed on one device, the transmitter, and only the parameters, but not the signal itself, are send to another device, the receiver. Now we want to reconstruct the speech signal only from the provided parameters with the help of a LPC-vocoder. We will proceed step-by-step, incorporating one parameter a time, analyzing the influence of each of the parameters to the overall result.\n",
    "\n",
    "** 3.1 LPCs and constant excitation signal** \n",
    "\n",
    "First, we want to utilize only the LPCs, modeling the influence of the vocal tract. For this, we filter a stationary\n",
    "excitation signal e(n) with the LPC coefficients.\n",
    "\n",
    "- Create two excitation signals\n",
    "    1. unvoiced: white Gaussian noise via np.random.randn\n",
    "    2. voiced: pulse train with a frequency of 100 Hz.\n",
    "        Tip: `voiced_ex = np.zeros(num_samples) and voiced_ex[::fund_period_in_samples] = 1`\n",
    "- Segment the excitation signal e(n) into non-overlapping frames of length R (the same segmentation will be used\n",
    "implicitly in all upcoming experiments)\n",
    "- Filter each segment with the corresponding LPCs using the function filter_adaptively provided by the package\n",
    "downloaded from STiNE. It allows for improved adaptive filtering by introducing a filter state storing necessary\n",
    "information of the last filtering. It can be used as\n",
    "segment_out , filter_state = filter_adaptively(np.array ([1]), LPCs , segment ,\n",
    "filter_state_in).\n",
    "Note that filter_state from the last segment is used as an input parameter. It is then modified by filter_adaptively\n",
    "and the new filter_state is returned.\n",
    "- Put together the separate segments to obtain the whole signal.\n",
    "- Listen to both of the signals, voiced only and unvoiced only. Can you understand what is said?\n",
    "- Display the spectrogram of the two synthesized signals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "signal_length_s = len(y) / sr \n",
    "num_samples = sr * signal_length_s  # 1 second of signal\n",
    "M = 16  # LPC order\n",
    "\n",
    "print(f\"Sampling Rate: {sr} kHz\")\n",
    "print(f\"Signal Length: {signal_length_s} s\")\n",
    "print(f\"Total Samples: {num_samples}\")\n",
    "print(f\"Fundamental Period in Samples: {sr/100}\")\n",
    "\n",
    "# Generate excitation signals\n",
    "def create_excitation_signals(num_samples, fs):\n",
    "    # Unvoiced excitation: White Gaussian noise\n",
    "    unvoiced_ex = np.random.randn(num_samples)\n",
    "    \n",
    "    # Voiced excitation: Pulse train with a frequency of 100 Hz\n",
    "    fund_freq = 100  # Fundamental frequency in Hz\n",
    "    fund_period = int(fs / fund_freq)  # Fundamental period in samples\n",
    "    voiced_ex = np.zeros(num_samples)\n",
    "    voiced_ex[::fund_period] = 1\n",
    "    \n",
    "    return unvoiced_ex, voiced_ex\n",
    "\n",
    "\n",
    "\n",
    "# Synthesize signal using LPCs and excitation signal\n",
    "def synthesize_signal(excitation_signal, m_lpc, segment_length):\n",
    "    num_segments = m_lpc.shape[0]\n",
    "    filter_state = None\n",
    "    synthesized_segments = []\n",
    "    \n",
    "    for i in range(num_segments):\n",
    "        segment = excitation_signal[i*segment_length:(i+1)*segment_length]\n",
    "        lpc = np.concatenate(([1], -m_lpc[i]))\n",
    "        segment_out, filter_state = filter_adaptively(np.array([1]), lpc, segment, filter_state)\n",
    "        synthesized_segments.append(segment_out)\n",
    "    \n",
    "    return np.concatenate(synthesized_segments)\n",
    "\n",
    "# Main process\n",
    "# Generate excitation signals\n",
    "unvoiced_ex, voiced_ex = create_excitation_signals(num_samples, sr)\n",
    "\n",
    "# Parameters for segmentation\n",
    "R = 32\n",
    "\n",
    "# Segment the excitation signals\n",
    "unvoiced_segments, v_time_frame_u = my_windowing(unvoiced_ex, sr, R, R)\n",
    "voiced_segments, v_time_frame_v = my_windowing(voiced_ex, sr, R, R)\n",
    "\n",
    "\n",
    "# Print shapes of the segmented excitation signals to verify\n",
    "print(\"Unvoiced segments shape:\", unvoiced_segments.shape)\n",
    "print(\"Voiced segments shape:\", voiced_segments.shape)\n",
    "\n",
    "# Windowing function to segment the input signal\n",
    "m_frames, v_time_frame = my_windowing(y, sr, frame_length, frame_shift)\n",
    "\n",
    "# Compute LPC coefficients\n",
    "m_lpc = compute_lpc(m_frames, M)\n",
    "\n",
    "# Synthesize signals\n",
    "unvoiced_synthesized = synthesize_signal(unvoiced_ex, m_lpc, R)\n",
    "voiced_synthesized = synthesize_signal(voiced_ex, m_lpc, R)\n",
    "\n",
    "# Function to plot spectrogram\n",
    "def plot_spectrogram(signal, fs, title):\n",
    "    f, t, Sxx = stft(signal, fs=fs, nperseg=256, noverlap=128)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.pcolormesh(t, f, np.abs(Sxx), shading='gouraud')\n",
    "    plt.title(title)\n",
    "    plt.ylabel('Frequency [Hz]')\n",
    "    plt.xlabel('Time [s]')\n",
    "    plt.colorbar(label='Magnitude')\n",
    "    plt.show()\n",
    "\n",
    "# Plot spectrograms\n",
    "plot_spectrogram(unvoiced_synthesized, sr, 'Unvoiced Synthesized Signal Spectrogram')\n",
    "plot_spectrogram(voiced_synthesized, sr, 'Voiced Synthesized Signal Spectrogram')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listen to the signals (requires sounddevice)\n",
    "import sounddevice as sd\n",
    "print(\"Playing unvoiced synthesized signal...\")\n",
    "sd.play(unvoiced_synthesized, sr)\n",
    "sd.wait()\n",
    "print(\"Playing voiced synthesized signal...\")\n",
    "sd.play(voiced_synthesized, sr)\n",
    "sd.wait()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
