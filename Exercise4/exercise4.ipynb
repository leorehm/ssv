{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import get_window, stft\n",
    "from scipy.linalg import solve_toeplitz\n",
    "import sounddevice as sd\n",
    "\n",
    "from util import my_windowing, compute_stft, plot_log_magnitude\n",
    "from filteradaptively import filter_adaptively\n",
    "\n",
    "\n",
    "%matplotlib widget\n",
    "plt.close(\"all\")\n",
    "\n",
    "file = \"Audio/female8khz.wav\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, sr = librosa.load(file, sr=None)\n",
    "print(f\"Sampling Rate: {sr} Hz\")\n",
    "print(f\"Signal Length: {len(y) / sr} s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.1 Segmentation**\n",
    "\n",
    "Before we start to analyze the speech signal, we first split the signal into overlapping segments with a length of N samples. Each segment overlaps with the previous frame by $L = N−R$ samples, like depicted in Fig. 1. Use a segment length of 32 ms and a segment shift of 8 ms. You may use the function `m_frames, v_time_frame = my_windowing(x, fs, N, R)` from the first exercise session.\n",
    "\n",
    "*Why do we segment the signal prior to analysis instead of processing the whole signal at once?*\n",
    "\n",
    "Time-Frequency Analysis:\n",
    "\n",
    "- Techniques like STFT require dividing the signal into frames.\n",
    "- Allows analysis of frequency content changes over time.\n",
    "- Crucial for understanding and processing speech signals.\n",
    "\n",
    "Dynamic Range:\n",
    "\n",
    "- Speech signals have varying dynamic ranges and temporal variations.\n",
    "- Segmenting isolates these variations for separate analysis.\n",
    "- Leads to better feature extraction and enhancement.\n",
    "\n",
    "*Is a segment length of 32 ms appropriate? Why or why not?*\n",
    "\n",
    " A 32 ms window provides a good balance between time resolution and frequency resolution. Shorter windows offer better time resolution but poorer frequency resolution, while longer windows offer better frequency resolution but poorer time resolution. A 32 ms window is a common compromise used in speech processing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_length = 32\n",
    "frame_shift = 8\n",
    "m_frames, v_time_frame = my_windowing(y, sr, 32, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.2 Signal power**\n",
    "\n",
    "In the following, we want to estimate a set of parameters for each of the segments. We begin with creating a function named compute_power that takes a signal segment as an input variable and returns the signal power,\n",
    "\n",
    "where n is the sample index and x(n) corresponds to the current signal segment. Compute the signal power for each segment and store it in a vector. Display the standard deviation (√σ2x = σx) together with the waveform of the speech signal in one plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_power(seg: np.ndarray) -> float:\n",
    "    return (1/len(seg)) * np.sum(seg**2)\n",
    "\n",
    "compute_power(m_frames[20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.3 Voiced / unvoiced decision**\n",
    "\n",
    "Write another function, `is_voiced`, to also analyze if a segment is voiced or not. Here, we use a simple approach, which is counting the number of zero crossings within in one segment.\n",
    "\n",
    "Definition:  \n",
    "> The zero-crossing rate (ZCR) is the rate at which a signal changes from positive to zero to negative or from negative to zero to positive.\n",
    "\n",
    "- Count the number of zero crossings in each segment and normalize them by the segment length $N$. Apply a threshold to decide if the segment is voiced or not. Try to find an appropriate value for the threshold that allows for a reliable classification. Do not expect it to be 100% perfect, but try your best.\n",
    "- `is_voiced` should return 1 for voiced segments and 0 for unvoiced segments\n",
    "\n",
    "Explanation:\n",
    "- We use np.diff(np.sign(segment)) to find the points where the sign of the segment changes, indicating a zero crossing. np.sum(np.abs(...) > 0) counts these crossings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_voiced(seg: np.ndarray, threshold: float = 0.3) -> int:\n",
    "    zero_crossings = np.sum(np.abs(np.diff(np.sign(seg))) > 0)\n",
    "    zero_crossing_rate = zero_crossings / len(seg)\n",
    "    return 1 if zero_crossing_rate < threshold else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = np.arange(0, len(y)) / sr\n",
    "\n",
    "v_uv_frames = np.asarray([is_voiced(x, 0.3) for x in m_frames])\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "fig.set_figheight(4)\n",
    "fig.set_figwidth(10)\n",
    "\n",
    "ax1.set_ylabel(\"time [s]\")\n",
    "ax1.set_xlabel(\"Amplitude\")\n",
    "\n",
    "ax1.plot(time, y)\n",
    "ax1.plot(v_time_frame, v_uv_frames*0.1)\n",
    "\n",
    "plot_log_magnitude(*compute_stft(y, sr, frame_length, frame_shift, get_window(\"hann\", int(sr * frame_length / 1000), True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Why might the number of zero crossings provide valuable information for the voiced/unvoiced decision?*\n",
    "\n",
    "Voiced Sounds:\n",
    "- Have a more regular, periodic waveform with fewer zero crossings.\n",
    "\n",
    "Unvoiced Sounds:\n",
    "- Have a more irregular, noisy waveform with more zero crossings.\n",
    "\n",
    "Zero Crossings:\n",
    "- Counting zero crossings helps distinguish between the regularity of voiced sounds and the irregularity of unvoiced sounds.\n",
    "\n",
    "*In general: are all speech sounds either voiced or unvoiced? Can you think of other speech sounds? Examples?*\n",
    "- Whispered Speech\n",
    "- Clicks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.4 Fundamental frequency estimation**\n",
    "\n",
    "In voiced sounds, the vocal cords periodically open and close. The frequency at which they open and close is called the fundamental frequency, $f_0$.\n",
    "\n",
    "You have already implemented a method to estimate the fundamental frequency based on the autocorrelation function (ACF) in the first exercise session. We can now reuse it here. Put your estimator into a Python function of the form `def estimate_f0(m_frames: np.ndarray, fs: int) -> [np.ndarray (v_f0_estimates)]` where `v_f0_estimates` is a vector containing the fundamental frequency estimates in [Hz] for each signal segment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_f0(m_frames: np.ndarray, fs: int) -> np.ndarray:\n",
    "    fundamental_freqs = []\n",
    "    \n",
    "    # Step b: Compute ACF for each frame and step d: Estimate fundamental frequency\n",
    "    for frame in m_frames:\n",
    "        # ACF via convolution\n",
    "        acf = np.convolve(frame, frame[::-1], mode='full')\n",
    "        \n",
    "        # Step c: Focus on non-negative lags\n",
    "        acf = acf[len(acf)//2:]\n",
    "        \n",
    "        # Convert frequency range to corresponding lag range\n",
    "        min_lag = fs // 400  # Maximum frequency\n",
    "        max_lag = fs // 80   # Minimum frequency\n",
    "                \n",
    "        # Step d: Identify peak within the lag range corresponding to 80 Hz to 400 Hz\n",
    "        peak_lag = np.argmax(acf[min_lag:max_lag]) + min_lag\n",
    "        fundamental_freq = fs / peak_lag\n",
    "        \n",
    "        fundamental_freqs.append(fundamental_freq)\n",
    "    \n",
    "    return np.asarray(fundamental_freqs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot your fundamental frequency estimate together with the spectrogram of the clean speech. For this, the y-axis needs to be in Hz. Use the function `compute_stft` from exercise 2 to compute the spectrogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_stft, v_freq, v_time = compute_stft(y, sr, frame_length, frame_shift, get_window(\"hann\", int(sr * frame_length / 1000), True))\n",
    "v_f0 = estimate_f0(m_frames, sr)\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.set_figwidth(12)\n",
    "ax = fig.add_subplot(111)\n",
    "ax.set_ylabel(\"frequency [Hz]\")\n",
    "ax.set_xlabel(\"time [s]\")\n",
    "im = ax.imshow(\n",
    "    10 * np.log10(np.maximum(np.square(np.abs(m_stft.T)), 10 ** (-15))),\n",
    "    cmap=\"viridis\",\n",
    "    origin=\"lower\",\n",
    "    extent=[v_time[0], v_time[-1], v_freq[0], v_freq[-1]],\n",
    "    aspect=\"auto\",\n",
    ")\n",
    "ax.plot(v_time, v_f0, \"r\", linewidth=0.8)\n",
    "colorbar = fig.colorbar(im, orientation=\"vertical\", pad=0.05)\n",
    "colorbar.set_label(\"dB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.5 Linear prediction coefficients / linear predictive coding**\n",
    "\n",
    "In the lecture, you learned that the form of the vocal tract is crucial to speech production and that it can be modeled by concatenating sections of lossless tubes. This model can well be described in terms of reflection coefficients for each section border. A closely related description can be obtained in terms of an all-pole filter, i.e. its filter coefficients, known as the linear prediction coefficients (LPCs). A common, since very efficient, method to compute the LPCs is the so-called Levinson-Durbin algorithm. Although not necessary at this point, it is worth mentioning that LPCs can be transformed into reflection coefficients and vice versa. This will come in handy in Section 4.\n",
    "- Shortly outline the Source-Filter-Model for speech production.\n",
    "- Compute the LPCs of the signal segments based on the code you have from exercise session three. For that, write a Python function `def compute_lpc(m_frames: np.ndarray, M: int)->[np.ndarray (m_lpc)]` that returns the LP coefficients of each signal segment in the matrix `m_lpc`, which is of size `[num_frames x M]`.\n",
    "- Choose a suitable model order $M$ for linear prediction for a signal with an audio bandwidth of 4 kHz. Give reasons for your choice.\n",
    "\n",
    "Model order $M=12$, because TODO\n",
    "\n",
    "Now we have all parameters we need to decently describe our speech signal. In the next section, we will try to reconstruct the actual waveform of the speech signal based only on this parameter set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3\n",
    "def get_coefficients(signal: np.ndarray, lpc_order: int = 12) -> np.ndarray:\n",
    "    # compute the autocorrelation vector\n",
    "    phi_full = np.correlate(signal, signal, mode=\"full\")  # mode \"valid\" | \"full\"\n",
    "    # autocorrelation is symmetric, we only need one half\n",
    "    center = len(phi_full) // 2\n",
    "    phi = phi_full[center:center + lpc_order + 1]\n",
    "    # The right-hand side vector is the remaining part\n",
    "    b = phi[1:]\n",
    "    # Solve for the LPC coefficients\n",
    "    a = solve_toeplitz((phi[:-1], phi[:-1]), b)\n",
    "\n",
    "    return -a\n",
    "\n",
    "def compute_lpc(m_frames: np.ndarray, M: int = 12) -> np.ndarray:\n",
    "    return np.asarray([get_coefficients(frame, M) for frame in m_frames])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3 LPC-vocoder: synthesis**\n",
    "\n",
    "In this section, lets assume that the speech signal is recorded and analyzed on one device, the transmitter, and only the parameters, but not the signal itself, are send to another device, the receiver. Now we want to reconstruct the speech signal only from the provided parameters with the help of a LPC-vocoder. We will proceed step-by-step   incorporating one parameter a time, analyzing the influence of each of the parameters to the overall result.\n",
    "\n",
    "**3.1 LPCs and constant excitation signal** \n",
    "\n",
    "First, we want to utilize only the LPCs, modeling the influence of the vocal tract. For this, we filter a stationary\n",
    "excitation signal `e(n)` with the LPC coefficients.\n",
    "\n",
    "- Create two excitation signals\n",
    "    1. unvoiced: white Gaussian noise via np.random.randn\n",
    "    2. voiced: pulse train with a frequency of 100 Hz.\n",
    "\n",
    "        Tip: `voiced_ex = np.zeros(num_samples)` and `voiced_ex[::fund_period_in_samples] = 1`\n",
    "        \n",
    "- Segment the excitation signal e(n) into non-overlapping frames of length R (the same segmentation will be used\n",
    "implicitly in all upcoming experiments)\n",
    "- Filter each segment with the corresponding LPCs using the function filter_adaptively provided by the package\n",
    "downloaded from STiNE. It allows for improved adaptive filtering by introducing a filter state storing necessary\n",
    "information of the last filtering. It can be used as \n",
    "\n",
    "    `segment_out , filter_state = filter_adaptively(np.array ([1]), LPCs, segment, filter_state_in)`.\n",
    "\n",
    "    Note that `filter_state` from the last segment is used as an input parameter. It is then modified by `filter_adaptively` and the new `filter_state` is returned.\n",
    "- Put together the separate segments to obtain the whole signal.\n",
    "- Listen to both of the signals, voiced only and unvoiced only. Can you understand what is said?\n",
    "- Display the spectrogram of the two synthesized signals.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling Rate: 8000 Hz\n",
      "Signal Length: 2.25 s\n",
      "Total Samples: 18000\n",
      "Fundamental Period in Samples: 80\n",
      "\n",
      "Unvoiced segments shape: (282, 64)\n",
      "Voiced segments shape: (282, 64)\n"
     ]
    }
   ],
   "source": [
    "# Exercise 3\n",
    "def get_coefficients(signal: np.ndarray, lpc_order: int = 12) -> np.ndarray:\n",
    "    # compute the autocorrelation vector\n",
    "    phi_full = np.correlate(signal, signal, mode=\"full\")  # mode \"valid\" | \"full\"\n",
    "    # autocorrelation is symmetric, we only need one half\n",
    "    center = len(phi_full) // 2\n",
    "    phi = phi_full[center:center + lpc_order + 1]\n",
    "    # The right-hand side vector is the remaining part\n",
    "    b = phi[1:]\n",
    "    # Solve for the LPC coefficients\n",
    "    a = solve_toeplitz((phi[:-1], phi[:-1]), b)\n",
    "    return -a\n",
    "\n",
    "def compute_lpc(m_frames: np.ndarray, sr: int, M) -> np.ndarray:\n",
    "    # Get Hann window\n",
    "    frame_length = len(m_frames[0])\n",
    "    window = get_window(\"hann\", frame_length, True)\n",
    "\n",
    "    return np.asarray([get_coefficients(frame, M) for frame in m_frames])\n",
    "\n",
    "# Wir nutzen die Sampling Rate und Länge des Input-Signals\n",
    "signal_length_s = len(y) / sr \n",
    "num_samples = int(sr * signal_length_s)\n",
    "M = 16  # LPC order\n",
    "\n",
    "print(f\"Sampling Rate: {sr} Hz\")\n",
    "print(f\"Signal Length: {signal_length_s} s\")\n",
    "print(f\"Total Samples: {num_samples}\")\n",
    "print(f\"Fundamental Period in Samples: {sr // 100}\")\n",
    "\n",
    "# Generate excitation signals\n",
    "def create_excitation_signals(num_samples: int, sr: int) -> tuple[np.ndarray, np.ndarray]:\n",
    "    # Unvoiced excitation: White Gaussian noise\n",
    "    unvoiced_ex = np.random.randn(num_samples)\n",
    "    \n",
    "    # Voiced excitation: Pulse train with a frequency of 100 Hz\n",
    "    fund_freq = 100  # Fundamental frequency in Hz\n",
    "    fund_period = int(sr / fund_freq)  # Fundamental period in samples\n",
    "    \n",
    "    voiced_ex = np.zeros(num_samples)\n",
    "    # wenn fund_period = 80, wird hier jedes 80. sample = 1 gesetzt\n",
    "    voiced_ex[::fund_period] = 1\n",
    "    \n",
    "    return unvoiced_ex, voiced_ex\n",
    "\n",
    "unvoiced_ex, voiced_ex = create_excitation_signals(num_samples, sr)\n",
    "\n",
    "# Split the excitations signals into 8 ms non-overlapping segments\n",
    "R = frame_shift\n",
    "\n",
    "m_frames_u, v_time_frame_u = my_windowing(unvoiced_ex, sr, R, R)\n",
    "m_frames_v, v_time_frame_v = my_windowing(voiced_ex, sr, R, R)\n",
    "\n",
    "# Print shapes of the segmented excitation signals to verify\n",
    "print(\"\\nUnvoiced segments shape:\", m_frames_u.shape)\n",
    "print(\"Voiced segments shape:\", m_frames_v.shape)\n",
    "\n",
    "# Compute LPC coefficients\n",
    "m_lpc_u = compute_lpc(m_frames_u, sr, M)\n",
    "# m_lpc_v = compute_lpc(m_frames_v, sr, M)\n",
    "\n",
    "def synthesize_signal(m_frames, m_lpc):\n",
    "    filtered_frames = []\n",
    "    filter_state = None\n",
    "\n",
    "    assert len(m_frames) == len(m_lpc)\n",
    "    num_frames = len(m_frames)\n",
    "\n",
    "    for i in range(num_frames):\n",
    "        segment_out, filter_state = filter_adaptively(\n",
    "            np.array([1]), \n",
    "            np.concatenate(([1], m_lpc[i])), \n",
    "            m_frames[i], \n",
    "            filter_state\n",
    "        )\n",
    "        filtered_frames.append(segment_out)\n",
    "    \n",
    "    # Put together the separate segements and return\n",
    "    return np.concatenate(filtered_frames)\n",
    "\n",
    "synthesized_u = synthesize_signal(m_frames_u, m_lpc_u)\n",
    "# synthesized_v = synthesize_signal(m_frames_v, m_lpc_v)\n",
    "\n",
    "# Synthesize signals\n",
    "# unvoiced_synthesized = synthesize_signal(unvoiced_ex, m_lpc, R)\n",
    "# voiced_synthesized = synthesize_signal(voiced_ex, m_lpc, R)\n",
    "\n",
    "# # Function to plot spectrogram\n",
    "# def plot_spectrogram(signal, fs, title):\n",
    "#     f, t, Sxx = stft(signal, fs=fs, nperseg=256, noverlap=128)\n",
    "#     plt.figure(figsize=(10, 6))\n",
    "#     plt.pcolormesh(t, f, np.abs(Sxx), shading='gouraud')\n",
    "#     plt.title(title)\n",
    "#     plt.ylabel('Frequency [Hz]')\n",
    "#     plt.xlabel('Time [s]')\n",
    "#     plt.colorbar(label='Magnitude')\n",
    "#     plt.show()\n",
    "\n",
    "# # Plot spectrograms\n",
    "# plot_spectrogram(unvoiced_synthesized, sr, 'Unvoiced Synthesized Signal Spectrogram')\n",
    "# plot_spectrogram(voiced_synthesized, sr, 'Voiced Synthesized Signal Spectrogram')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Playing unvoiced synthesized signal...\n"
     ]
    },
    {
     "ename": "PortAudioError",
     "evalue": "Error querying device -1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPortAudioError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Listen to the signals (requires sounddevice)\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlaying unvoiced synthesized signal...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[43msd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplay\u001b[49m\u001b[43m(\u001b[49m\u001b[43msynthesized_u\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m sd\u001b[38;5;241m.\u001b[39mwait()\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# print(\"Playing voiced synthesized signal...\")\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# sd.play(voiced_synthesized, sr)\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# sd.wait()\u001b[39;00m\n",
      "File \u001b[0;32m~/ssv/.venv/lib/python3.10/site-packages/sounddevice.py:175\u001b[0m, in \u001b[0;36mplay\u001b[0;34m(data, samplerate, mapping, blocking, loop, **kwargs)\u001b[0m\n\u001b[1;32m    172\u001b[0m     ctx\u001b[38;5;241m.\u001b[39mwrite_outdata(outdata)\n\u001b[1;32m    173\u001b[0m     ctx\u001b[38;5;241m.\u001b[39mcallback_exit()\n\u001b[0;32m--> 175\u001b[0m \u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[43mOutputStream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamplerate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_channels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mprime_output_buffers_using_stream_callback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m                 \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ssv/.venv/lib/python3.10/site-packages/sounddevice.py:2605\u001b[0m, in \u001b[0;36m_CallbackContext.start_stream\u001b[0;34m(self, StreamClass, samplerate, channels, dtype, callback, blocking, **kwargs)\u001b[0m\n\u001b[1;32m   2602\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstart_stream\u001b[39m(\u001b[38;5;28mself\u001b[39m, StreamClass, samplerate, channels, dtype, callback,\n\u001b[1;32m   2603\u001b[0m                  blocking, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   2604\u001b[0m     stop()  \u001b[38;5;66;03m# Stop previous playback/recording\u001b[39;00m\n\u001b[0;32m-> 2605\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream \u001b[38;5;241m=\u001b[39m \u001b[43mStreamClass\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamplerate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msamplerate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2606\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mchannels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchannels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2607\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2608\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2609\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mfinished_callback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfinished_callback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2610\u001b[0m \u001b[43m                              \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2611\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream\u001b[38;5;241m.\u001b[39mstart()\n\u001b[1;32m   2612\u001b[0m     \u001b[38;5;28;01mglobal\u001b[39;00m _last_callback\n",
      "File \u001b[0;32m~/ssv/.venv/lib/python3.10/site-packages/sounddevice.py:1502\u001b[0m, in \u001b[0;36mOutputStream.__init__\u001b[0;34m(self, samplerate, blocksize, device, channels, dtype, latency, extra_settings, callback, finished_callback, clip_off, dither_off, never_drop_input, prime_output_buffers_using_stream_callback)\u001b[0m\n\u001b[1;32m   1472\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, samplerate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, blocksize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1473\u001b[0m              device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, latency\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1474\u001b[0m              extra_settings\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, finished_callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1475\u001b[0m              clip_off\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dither_off\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, never_drop_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1476\u001b[0m              prime_output_buffers_using_stream_callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   1477\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"PortAudio output stream (using NumPy).\u001b[39;00m\n\u001b[1;32m   1478\u001b[0m \n\u001b[1;32m   1479\u001b[0m \u001b[38;5;124;03m    This has the same methods and attributes as `Stream`, except\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1500\u001b[0m \n\u001b[1;32m   1501\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1502\u001b[0m     \u001b[43m_StreamBase\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkind\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moutput\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwrap_callback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43marray\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1503\u001b[0m \u001b[43m                         \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_remove_self\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlocals\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ssv/.venv/lib/python3.10/site-packages/sounddevice.py:825\u001b[0m, in \u001b[0;36m_StreamBase.__init__\u001b[0;34m(self, kind, samplerate, blocksize, device, channels, dtype, latency, extra_settings, callback, finished_callback, clip_off, dither_off, never_drop_input, prime_output_buffers_using_stream_callback, userdata, wrap_callback)\u001b[0m\n\u001b[1;32m    822\u001b[0m         samplerate \u001b[38;5;241m=\u001b[39m isamplerate\n\u001b[1;32m    823\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    824\u001b[0m     parameters, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dtype, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_samplesize, samplerate \u001b[38;5;241m=\u001b[39m \\\n\u001b[0;32m--> 825\u001b[0m         \u001b[43m_get_stream_parameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkind\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchannels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlatency\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    826\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mextra_settings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamplerate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    827\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_device \u001b[38;5;241m=\u001b[39m parameters\u001b[38;5;241m.\u001b[39mdevice\n\u001b[1;32m    828\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_channels \u001b[38;5;241m=\u001b[39m parameters\u001b[38;5;241m.\u001b[39mchannelCount\n",
      "File \u001b[0;32m~/ssv/.venv/lib/python3.10/site-packages/sounddevice.py:2683\u001b[0m, in \u001b[0;36m_get_stream_parameters\u001b[0;34m(kind, device, channels, dtype, latency, extra_settings, samplerate)\u001b[0m\n\u001b[1;32m   2680\u001b[0m     samplerate \u001b[38;5;241m=\u001b[39m default\u001b[38;5;241m.\u001b[39msamplerate\n\u001b[1;32m   2682\u001b[0m device \u001b[38;5;241m=\u001b[39m _get_device_id(device, kind, raise_on_error\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m-> 2683\u001b[0m info \u001b[38;5;241m=\u001b[39m \u001b[43mquery_devices\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2684\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m channels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2685\u001b[0m     channels \u001b[38;5;241m=\u001b[39m info[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m kind \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_channels\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/ssv/.venv/lib/python3.10/site-packages/sounddevice.py:569\u001b[0m, in \u001b[0;36mquery_devices\u001b[0;34m(device, kind)\u001b[0m\n\u001b[1;32m    567\u001b[0m info \u001b[38;5;241m=\u001b[39m _lib\u001b[38;5;241m.\u001b[39mPa_GetDeviceInfo(device)\n\u001b[1;32m    568\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m info:\n\u001b[0;32m--> 569\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PortAudioError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError querying device \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    570\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m info\u001b[38;5;241m.\u001b[39mstructVersion \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m    571\u001b[0m name_bytes \u001b[38;5;241m=\u001b[39m _ffi\u001b[38;5;241m.\u001b[39mstring(info\u001b[38;5;241m.\u001b[39mname)\n",
      "\u001b[0;31mPortAudioError\u001b[0m: Error querying device -1"
     ]
    }
   ],
   "source": [
    "# Listen to the signals (requires sounddevice)\n",
    "print(\"Playing unvoiced synthesized signal...\")\n",
    "sd.play(synthesized_u, sr)\n",
    "sd.wait()\n",
    "\n",
    "# print(\"Playing voiced synthesized signal...\")\n",
    "# sd.play(voiced_synthesized, sr)\n",
    "# sd.wait()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
