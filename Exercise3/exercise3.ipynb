{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "from scipy.signal import get_window, freqz, lfilter\n",
    "from scipy.linalg import solve_toeplitz\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from util import plot_signal, compute_stft, plot_log_magnitude\n",
    "%matplotlib widget\n",
    "\n",
    "\n",
    "file = \"speech1.wav\"\n",
    "\n",
    "plt.close(\"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Load the speech file speech1.wav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, sr = librosa.load(file, sr=None)\n",
    "\n",
    "plot_signal(y, sr)\n",
    "\n",
    "v_hann_window = get_window(\"hann\", int(sr * 32 / 1000), True)\n",
    "plot_log_magnitude(*compute_stft(y, sr, 32, 8, v_hann_window))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Select one unvoiced and one voiced speech segment from the signal, each with a length of 32 ms. You may reuse\n",
    "your segmentation from Assignment 3 of Exercise 1 and/or your knowledge about the speech signal obtained in\n",
    "assignment 1b) of Exercise 1. Apply a Hann window of the same length to both segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_length_ms = 32\n",
    "\n",
    "def get_segment(y: np.ndarray, t0_ms: int, length_ms: int = 32) -> np.ndarray:\n",
    "    # convert ms to samples\n",
    "    length = int(sr * length_ms / 1000)\n",
    "    t0 = int(sr * t0_ms / 1000)\n",
    "    t1 = t0 + length\n",
    "    \n",
    "    # get segment\n",
    "    v_seg = y[t0:t1]\n",
    "    \n",
    "    return v_seg\n",
    "\n",
    "v_hann_window = get_window(\"hann\", int(sr * segment_length_ms / 1000), True)\n",
    "\n",
    "voiced_frame = get_segment(y, 300)\n",
    "unvoiced_frame = get_segment(y, 550)\n",
    "\n",
    "windowed_voiced_frame = voiced_frame * v_hann_window\n",
    "windowed_unvoiced_frame = unvoiced_frame * v_hann_window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Compute the $M = 12$-order LP coefficients by solving equation 4. Use the functions `np.correlate` and `scipy.linalg.solve_toeplitz` to compute the autocorrelation vector $\\phi_s$ and the correlation matrix $R_s$ respectively. Store the coefficients in a vector $a$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uses Burg's Algrorithm instead of the autocorrelation method\n",
    "print(\"Control: Librosa Linear Prediction Coefficients (voiced segment)\\n\", librosa.lpc(windowed_voiced_frame, order=12))\n",
    "print(\"Control: Librosa Linear Prediction Coefficients (voiced segment)\\n\", librosa.lpc(windowed_unvoiced_frame, order=12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coefficients(signal: np.ndarray, lpc_order: int = 12) -> np.ndarray:\n",
    "    # compute the autocorrelation vector\n",
    "    phi_full = np.correlate(signal, signal, mode=\"full\")  # mode \"valid\" | \"full\"\n",
    "\n",
    "    # autocorrelation is symmetric, we only need one half\n",
    "    center = len(phi_full) // 2\n",
    "    phi = phi_full[center:center + lpc_order + 1]\n",
    "\n",
    "    # The right-hand side vector is the remaining part\n",
    "    b = phi[1:]\n",
    "\n",
    "    # Solve for the LPC coefficients\n",
    "    a = solve_toeplitz((phi[:-1], phi[:-1]), b)\n",
    "\n",
    "    return -a\n",
    "\n",
    "a_voiced = get_coefficients(windowed_voiced_frame)\n",
    "print(\"Linear Prediction Coefficients (voiced segment):\\n\", a_voiced)\n",
    "\n",
    "a_unvoiced = get_coefficients(windowed_unvoiced_frame)\n",
    "print(\"Linear Prediction Coefficients (unvoiced segment):\\n\", a_unvoiced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4a) Make a plot of the frequency response (amplitude as well as phase) of the estimated vocal tract filter $H(z)$ for both, the unvoiced and the voiced speech segment. To do this, the command `scipy.signal.freqz(1, np.concatenate(([1], a)), numPoints, whole=True, fs=sampling_freq)` might be helpful. For the number of frequency-points (`numPoints`), use the segment length in samples. Make sure that the axis descriptions of your plots is meaningful.\n",
    "\n",
    "4b) Why do you use `np.concatenate(([1], a))` and not only a?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_freq_response(a: np.ndarray, num_points: int, title: str = \"\", plot_phase: bool = True):\n",
    "    # compute the frequency response of the digital filter\n",
    "    #   w: the frequencies at which h was computed\n",
    "    #   h: the frequency response, as complex numbers.\n",
    "    w, h = freqz(1, np.concatenate(([1], a)), num_points, whole=True, fs=sr)\n",
    "    \n",
    "    fig, ax1 = plt.subplots()\n",
    "    fig.set_figwidth(12)\n",
    "    ax1.set_title(f\"Frequency Response ({title})\")\n",
    "\n",
    "    # plot amplidute \n",
    "    ax1.set_xlabel(\"Frequency [Hz]\")\n",
    "    ax1.set_ylabel(\"Amplitude\")\n",
    "    ax1.plot(w, np.abs(h), \"b\", label=\"Amplitude\")\n",
    "\n",
    "    # plot phase\n",
    "    if plot_phase:\n",
    "        ax2 = ax1.twinx()  # Instantiate a second y-axis that shares the same x-axis\n",
    "        ax2.set_ylabel(\"Phase [radians]\")\n",
    "        ax2.plot(w, np.angle(h), \"g\", label=\"Phase\")\n",
    "\n",
    "    fig.legend()\n",
    "\n",
    "num_points = int(sr * 32 / 1000)\n",
    "\n",
    "plot_freq_response(a_voiced, num_points, \"voiced\")\n",
    "plot_freq_response(a_unvoiced, num_points, \"unvoiced\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Compute the discrete Fourier transform (DFT) of the windowed segments using `S = np.fft.rfft(...)`. Plot the\n",
    "amplitude of $S$ in $dB$ together with the amplitude of the corresponding filter $H(z)$ in $dB$ in one plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dft_amplitude(y: np.ndarray, a: np.ndarray, num_points: int, title: str = \"\"):\n",
    "    S = np.fft.rfft(y)\n",
    "    S_freq = np.fft.rfftfreq(num_points, 1/sr)\n",
    "\n",
    "    w, h = freqz(1, np.concatenate(([1], a)), num_points, whole=True, fs=sr)\n",
    "\n",
    "    fig, ax1 = plt.subplots()\n",
    "    fig.set_figwidth(12)\n",
    "\n",
    "    ax1.set_title(f\"DFT & Filter Amplitude ({title})\")\n",
    "    ax1.set_xlabel(\"Frequency [Hz]\")\n",
    "    ax1.set_ylabel(\"Magnitude [dB]\")\n",
    "    ax1.plot(S_freq, 20*np.log10(np.abs(S)), \"b\", label=\"DFT\", linewidth=1)\n",
    "\n",
    "    ax1.plot(w, 20*np.log10(np.abs(h)), \"g\", label=\"Filter\", linewidth=1)\n",
    "\n",
    "    fig.legend()\n",
    "\n",
    "plot_dft_amplitude(windowed_voiced_frame, a_voiced, num_points, \"voiced\")\n",
    "plot_dft_amplitude(windowed_unvoiced_frame, a_unvoiced, num_points, \"unvoiced\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6a) For both segments, compute the residual signal by using the inverse filtering statement `e = scipy.signal.lfilter(np.concatenate(([1], a)), 1, s)`. Plot the residual signal $e$ together with the corresponding signal segment.**\n",
    "\n",
    "**b) Explain differences in $e$ between the voiced and unvoiced segment.**\n",
    "\n",
    "*The residual signal represents the part of the signal not captured by the model.*\n",
    "\n",
    "Voiced Segment\n",
    "- The residual signal of the voiced signal is very small / almost flat.\n",
    "- The voiced signal has a strong periodic nature with strong fundamental frequencies and harmonies, allowing the model to effectively capture the signal.\n",
    "\n",
    "Unvoiced Segment\n",
    "- The residual signal of the unvoiced segment is much more significant / of higher amplitude.\n",
    "- Unvoiced sounds are much more noise-like without clear periodic structure.\n",
    "- The sound is spread over a wider frequency range.\n",
    "- The model performs worse at capturing these signals.\n",
    "\n",
    "**c) Explain why `scipy.signal.lfilter(np.concatenate(([1], a)), 1, s)` yields the residual signal.**\n",
    "\n",
    "- The filter with the coefficients $[1, a_1, ..., a_i]$ is u sed to predict the signal based on its past samples.\n",
    "- Inverse filtering with the LPC coefficients removed the predicted part from the signal yielding the residual signal.\n",
    "- when using `scipy.signal.lfilter(np.concatenate(([1], a)), 1, s)` we apply the inverse filter to the signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_filter(signal: np.ndarray, a: np.ndarray, title:str = \"\"):\n",
    "    e = lfilter(np.concatenate(([1], a)), 1, signal)\n",
    "\n",
    "    fig, ax1 = plt.subplots()\n",
    "    fig.set_figwidth(12)\n",
    "\n",
    "    ax1.set_title(title)\n",
    "    ax1.set_xlabel(\"Time [s]\")\n",
    "    ax1.set_ylabel(\"Amplitude\")\n",
    "    ax1.plot(signal, \"b\", label=\"Original Signal\", linewidth=0.5)\n",
    "\n",
    "    ax1.plot(e, \"r\", label=\"Residual Signal\", linestyle=\"--\", linewidth=0.5)\n",
    "\n",
    "    fig.legend()\n",
    "    \n",
    "plot_filter(voiced_frame, a_voiced, \"voiced\")\n",
    "plot_filter(unvoiced_frame, a_unvoiced, \"unvoiced\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7a) Why are the logarithmic amplitudes of H and S (plots of assignment 5) not on the same level?**\n",
    "\n",
    "- The amplitude of the filter is typcially normalized.\n",
    "- The amplitude of the DFT is directly tied to the amplitude of the input signal.\n",
    "- The amplitude levels of the filter response are determined by the LPC coefficients and do not directly represent the absolute level of energy.\n",
    "\n",
    "**b) How can you modify H to achieve a better match? Hint: Experiment with the energy of the residual e.**\n",
    "\n",
    "- We could scale the filter response using the residual energy.\n",
    "- The root mean square (RMS) of a signal yields the average magnitude\n",
    "- Using the RMS of the residual signal $e$ and the DFT $S$ we can calclulate the scaling factor $sf = \\frac{RMS(e)}{RMS(S)}$\n",
    "\n",
    "**c) For the voiced speech segment, plot the amplitude of the modified filter H together with the amplitude of S in dB to check if your modification is correct.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dft_amplitude_scaled(frame: np.ndarray, windowed_frame: np.ndarray, a: np.ndarray, num_points: int, title: str = \"\"):\n",
    "    # calculate the DFT\n",
    "    S = np.fft.rfft(windowed_frame, num_points)\n",
    "    S_freq = np.fft.rfftfreq(num_points, 1/sr)\n",
    "\n",
    "    # calculate the frequency response \n",
    "    w, h = freqz(1, np.concatenate(([1], a)), num_points, whole=True, fs=sr)\n",
    "\n",
    "    # calculate offset to normalize the frequency respnse\n",
    "    # offset_s = np.sqrt(np.linalg.norm(S)**2 / len(S))\n",
    "    # offset_h = np.sqrt(np.linalg.norm(h)**2 / len(h))\n",
    "    \n",
    "    # apply filter to get residual energy\n",
    "    e = lfilter(np.concatenate(([1], a)), 1, frame)\n",
    "\n",
    "    # caclulate the RMS (root mean square)\n",
    "    rms_e = np.sqrt(np.mean(e**2))\n",
    "    rms_s = np.sqrt(np.mean(S**2))\n",
    "\n",
    "    # scale h using the scaling factor/RMS\n",
    "    h_scaled = h * (rms_e / rms_s)\n",
    "\n",
    "    fig, ax1 = plt.subplots()\n",
    "    fig.set_figwidth(12)\n",
    "\n",
    "    ax1.set_title(f\"DFT & Filter Amplitude, normalized ({title})\")\n",
    "    ax1.set_xlabel(\"Frequency [Hz]\")\n",
    "    ax1.set_ylabel(\"Magnitude [dB]\")\n",
    "    ax1.plot(S_freq, 20*np.log10(np.abs(S)), \"b\", label=\"DFT\", linewidth=1)\n",
    "\n",
    "    ax1.plot(w, 20*np.log10(np.abs(h_scaled)), \"g\", label=\"Filter\", linewidth=1)\n",
    "\n",
    "    fig.legend()\n",
    "\n",
    "# plot_dft_amplitude(windowed_voiced_frame, a_voiced, num_points, \"voiced\")\n",
    "plot_dft_amplitude_scaled(voiced_frame, windowed_voiced_frame, a_voiced, num_points, \"voiced\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Play with the order of the predictor $(M = 2 .. 20)$. Describe differences in $H(z)$ and explain reasons for that.\n",
    "\n",
    "When using higher order predictors, $H(z)$ has\n",
    "- higher precision/accuracy\n",
    "- frequencies with lower amplitude are becoming visible\n",
    "- reduced residual error (better prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 5, figsize=(18, 8))\n",
    "fig.suptitle(\"Frequency Response for M = 2..20\")\n",
    "\n",
    "axes = axes.flatten()\n",
    "\n",
    "ylim = (-10, 310)\n",
    "\n",
    "plot_idx = 0\n",
    "for i in range(2, 21, 2):\n",
    "    a_M = get_coefficients(windowed_voiced_frame, i)\n",
    "\n",
    "    w, h = freqz(1, np.concatenate(([1], a_M)), num_points, whole=True, fs=sr)\n",
    "\n",
    "    ax = axes[plot_idx]\n",
    "    ax.set_title(f\"M = {i}\")\n",
    "    ax.set_xlabel(\"Frequency [Hz]\")\n",
    "    ax.set_ylabel(\"Amplitude\")\n",
    "    ax.plot(w, np.abs(h), \"b\", label=\"Amplitude\", linewidth=0.8)\n",
    "    ax.set_ylim(ylim)\n",
    "    plot_idx += 1\n",
    "\n",
    "    \n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9\\. From the speech production model it is known that speech undergoes a spectral tilt of −6 dB/octave. To counteract this effect, a pre-emphasis filter of the following form is used:\n",
    "$$\n",
    "y(n) = s(n) −\\alpha s(n−1)\n",
    "$$\n",
    "\n",
    "**a) Compute the LP coefficients for the pre-emphasized voiced speech segment using function `scipy.signal.lfilter` and $\\alpha = 0.95$. Compare the results with and without pre-emphasis.**\n",
    "\n",
    "- The pre-emphasized signal shows less significant lower frequencies and more significant higher frequencies\n",
    "\n",
    "**b) What is the advantage of pre-emphasizing the speech signal?**\n",
    "\n",
    "- Increased precision of the higher frequencies of human speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y[n] -> y[n] - alpha * y[n-1]\n",
    "def pre_emphasis(signal: np.ndarray, alpha: float) -> np.ndarray:\n",
    "    return lfilter([1, -alpha], 1, signal)\n",
    "\n",
    "a_voiced = get_coefficients(windowed_voiced_frame, 12)\n",
    "\n",
    "pre_emphasized_segment = pre_emphasis(windowed_voiced_frame, 0.95)\n",
    "a_pre_emphasized = get_coefficients(pre_emphasized_segment, 12)\n",
    "\n",
    "# Print LP coefficients\n",
    "print(\"LP coefficients without pre-emphasis:\\n\", a_voiced)\n",
    "print(\"LP coefficients with pre-emphasis:\\n\", a_pre_emphasized)\n",
    "\n",
    "w, h = freqz(1, np.concatenate(([1], a_voiced)), num_points, whole=True, fs=sr)\n",
    "w_p, h_p = freqz(1, np.concatenate(([1], a_pre_emphasized)), num_points, whole=True, fs=sr)\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "fig.set_figwidth(12)\n",
    "ax1.set_title(f\"Frequency Response\")\n",
    "ax1.set_xlabel(\"Frequency [Hz]\")\n",
    "ax1.set_ylabel(\"Amplitude\")\n",
    "\n",
    "ax1.plot(w, np.abs(h), \"b\", label=\"w/o pre-emphasis\", linewidth=\"0.8\")\n",
    "ax1.plot(w_p, np.abs(h_p), \"r\", label=\"w/ pre-emphasis\", linewidth=\"0.8\")\n",
    "\n",
    "# ax2 = ax1.twinx()\n",
    "# ax2.plot(w_p, np.abs(h_p), \"g\", label=\"w/ pre-emphasis\", linewidth=\"0.8\")\n",
    "\n",
    "fig.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
