{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sounddevice as sd\n",
    "from scipy.signal import get_window\n",
    "\n",
    "from util import my_windowing, compute_istft\n",
    "\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the file Exercise6.zip from Moodle which contains two speech signals which are corrupted once by white noise and once by babble noise. For both files the signal-to-noise ratio (SNR) is 10 dB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WaveFile:\n",
    "    filename: str\n",
    "    y: np.ndarray\n",
    "    sr: int\n",
    "    num_samples: int\n",
    "\n",
    "    def __init__(self, filename: str) -> None:\n",
    "        self.filename = filename\n",
    "        self.y, self.sr = librosa.load(filename, sr=None)\n",
    "\n",
    "    def play(self):\n",
    "         sd.play(self.y, self.sr)\n",
    "\n",
    "    def _split_frames(self, frame_length, frame_shift):\n",
    "        self.m_frames, self.v_time = my_windowing(\n",
    "            self.y,\n",
    "            self.sr,\n",
    "            frame_length,\n",
    "            frame_shift\n",
    "        )\n",
    "        self.num_frames = self.m_frames.shape[0]\n",
    "        self.samples_per_frame = self.m_frames.shape[1]\n",
    "    \n",
    "    def compute_stft(self, frame_length: int, frame_shift: int, window: str):\n",
    "        self._split_frames(frame_length, frame_shift)\n",
    "        \n",
    "        v_analysis_window = np.sqrt(get_window(window=window, Nx=self.samples_per_frame, fftbins=True))\n",
    "\n",
    "        # create frequency vector\n",
    "        v_freq = self.sr * (np.arange(self.samples_per_frame / 2 + 1) / self.samples_per_frame)\n",
    "\n",
    "        # calculate DFT\n",
    "        _m_frames = self.m_frames\n",
    "        _m_frames *= v_analysis_window\n",
    "        dft = np.fft.fft(_m_frames, axis=1)\n",
    "\n",
    "        # drop upper half of the spectrum while keeping the nyquist frequency in the middle\n",
    "        num_freq_bins = dft.shape[1]\n",
    "        if num_freq_bins % 2 == 0: # even \n",
    "            dft = dft[:, : num_freq_bins // 2 + 1]\n",
    "        else: # odd\n",
    "            dft = dft[:, : (num_freq_bins + 1) // 2]\n",
    "        m_stft = dft\n",
    "        \n",
    "        np.testing.assert_array_almost_equal(m_stft, np.fft.rfft(_m_frames, axis=1))\n",
    "        \n",
    "        self.m_stft = m_stft.T\n",
    "        self.v_freq = v_freq\n",
    "\n",
    "    def set_spp(self, m_spp: np.ndarray):\n",
    "        self.m_spp = m_spp\n",
    "    \n",
    "    def set_psd(self, m_signal_psd:np.ndarray, m_noise_psd: np.ndarray):\n",
    "        self.m_signal_psd = m_signal_psd\n",
    "        self.m_noise_psd = m_noise_psd\n",
    "\n",
    "    def set_enhanced_stft(self, m_enhanced_stft: np.ndarray):\n",
    "        self.m_enhanced_stft = m_enhanced_stft\n",
    "\n",
    "\n",
    "speech_babble = WaveFile(\"SpeechBabble.wav\")\n",
    "speech_white = WaveFile(\"SpeechWhite.wav\")\n",
    "\n",
    "frame_length = 32\n",
    "frame_shift = 16\n",
    "\n",
    "speech_babble.compute_stft(frame_length, frame_shift, \"hann\")\n",
    "speech_white.compute_stft(frame_length, frame_shift, \"hann\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_babble.play()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_white.play()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Acronyms**\n",
    "\n",
    "- STFT - Short-time Fourier-Transformation\n",
    "- PSD - Power Spectral Density\n",
    "- SPP - Speech Presence Probability\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1 Noise Power Estimation**\n",
    "\n",
    "In this part of the exercise, the noise power is estimated based on the speech presence probability. First, use your functions from the second exercise to create the STFT of the noisy input signals. The frame length should be 32 ms and the frame shift 16 ms. Use a $\\sqrt{Hann}$-window as analysis window. From the STFT, compute the periodograms and determine the noise PSD by performing the following steps for each frame.\n",
    "\n",
    "1. Compute the posterior probability of speech presence via (1):\n",
    "$$\n",
    "P(H_1|Y[k,\\ell]) = (1 + (1 + \\theta) exp(-\\frac{|Y[k,\\ell]|^2}{\\hat\\sigma^2_n[k,\\ell-1]} \\frac{\\theta}{1+\\theta}))^{-1}\n",
    "$$\n",
    "\n",
    "Here, $|Y[k,\\ell]|^2$ denotes the periodogram of the noisy input signal and $\\hat\\sigma^2_n[k,\\ell]$ is the estimated PSD of the background noise. Furthermore, $k$ indexes the frequency bins and $\\ell$ the frames. The SNR between speech and noise which is assumed in speech presence is denoted by $\\theta$. This value should be set to 15 dB.\n",
    "\n",
    "2. In order to avoid stagnations, a smoothed posterior probability $Q[k,\\ell]$ is required which is given as (2):\n",
    "$$\n",
    "Q[k,\\ell] = 0.9 Q[k,\\ell-1] + 0.1 P(H_1|Y[k,\\ell])\n",
    "$$\n",
    "\n",
    "For every frequency bin in the current frame $\\ell$ for which $Q[k,\\ell] > 0.99$ holds, set your computed posterior $P(H_1|Y[k,\\ell])$ from equation (1) to $min(0.99, P(H_1|Y[k,\\ell]))$.\n",
    "\n",
    "3. Estimate the noise periodogram via (3):\n",
    "$$\n",
    "|\\hat N[k,\\ell]|^2 = P(H_1|Y[k,\\ell]) \\hat\\sigma^2_n[k,\\ell-1] + (1-P(H_1|Y[k,\\ell])) |Y[k,\\ell]|^2\n",
    "$$\n",
    "\n",
    "and update the noise power density with (4):\n",
    "$$\n",
    "\\hat\\sigma^2_n[k,\\ell] = 0.8 \\hat\\sigma^2_n[k,\\ell-1] + 0.2 |\\hat N[k,\\ell]|^2\n",
    "$$\n",
    "\n",
    "Store the estimated noise PSD and the speech presence probability for each frame in a row of the matrices `m_noise_psd` and `m_spp` as it may simplify solving the following questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ssp(f: WaveFile, snr: int):\n",
    "    m_signal_psd = np.abs(f.m_stft)**2\n",
    "    num_frames = m_signal_psd.shape[0]\n",
    "    num_bins = m_signal_psd.shape[1]\n",
    "    \n",
    "    m_spp = np.zeros((num_frames, num_bins))\n",
    "    m_noise_psd = np.zeros((num_frames, num_bins+1))\n",
    "    Q = np.zeros((num_frames, num_bins+1))\n",
    "\n",
    "    # init noise periodogram\n",
    "    m_noise_psd[:, 0] = np.abs(f.m_stft[:, 0])**2\n",
    "\n",
    "    # loop over frequency bins\n",
    "    for i in range(num_bins):\n",
    "        # speech presence probability (1)\n",
    "        m_spp[:, i] = (1 + (1 + snr) * np.exp(-(m_signal_psd[:, i] / m_noise_psd[:, i]) * (snr / (1 + snr))))**(-1)\n",
    "\n",
    "        # smoothed posterior probability Q (2)\n",
    "        Q[:, i+1] = 0.9 * Q[:, i] + 0.1 * m_spp[:, i]\n",
    "        # set computed posterior probability for each frequency bin\n",
    "        for j in range(num_frames):\n",
    "            if Q[j, i+1] > 0.99:\n",
    "                m_spp[j, i] = min(m_spp[j, i], 0.99)\n",
    "\n",
    "        # estimate the noise periodogram (3)\n",
    "        noise_periodogram = m_spp[:, i] * m_noise_psd[:, i] + (1 - m_spp[:, i]) * m_signal_psd[:, i]\n",
    "        \n",
    "        # update noise power density (4)\n",
    "        m_noise_psd[:, i+1] = 0.8 * m_noise_psd[:, i] + 0.2*noise_periodogram\n",
    "\n",
    "    f.set_spp(m_spp)\n",
    "    f.set_psd(m_signal_psd, m_noise_psd)\n",
    "\n",
    "compute_ssp(speech_babble, 15)\n",
    "compute_ssp(speech_white, 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions**\n",
    "\n",
    "1.1 In equation (1), (3) and (4) the noise power estimate from the previous frame, $\\hat\\sigma^2_n[k, \\ell-1]$, is required. However, there is none available for the first frame $\\ell=0$. The same problem occurs for the smoothed speech presence probability $Q[k,\\ell]$ in (2).\n",
    "\n",
    "a) What would be appropriate initializations for the first noise estimate $\\hat\\sigma^2_n[k, \\ell-1]$ and the smoothed posterior probability $Q[k,\\ell−1]$? Explain briefly why you chose your initialization method.\n",
    "\n",
    "- For the first noise estimate $\\hat\\sigma^2_n[k, \\ell-1]$ we use first frame of the noisy input signal $|Y[k,\\ell]|^2$. This way the fraction in (1) will evalutate to 1.\n",
    "- The first frame of $Q[k,\\ell−1]$ is initalized with 0. \n",
    "\n",
    "1.2 Plot the speech presence probability $P(H_1|Y[k,\\ell])$ using `plt.imshow`.\n",
    "\n",
    "a) Which values do you obtain for time-frequency points where speech is present?\n",
    "- 1\n",
    "\n",
    "b) What values do you get for time-frequency points where only noise is present?\n",
    "- 0\n",
    "\n",
    "c) If you compare the speech presence probability with the spectrogram of your input signal, can you see\n",
    "similarities?\n",
    "- the original signal can be identified in the SPP spectogram\n",
    "- since babble noise is made up of speech, the SPP spectogram appears noisier\n",
    "\n",
    "1.3 Plot the estimated noise PSD as a spectrogram using plt.imshow.\n",
    "\n",
    "a) How well is the background noise estimated?\n",
    "- fairly well\n",
    "    - babble noise can be seen in the expected frequency spectrum of human speech\n",
    "    - white noise can be seen across the entire spectrum\n",
    "\n",
    "b) Can you observe errors (e. g. components that do not belong to the background noise)?\n",
    "- high probablity at specific frequencies early in the time domain (left side of the spectogram)\n",
    "\n",
    "c) What would be the consequence of such errors?\n",
    "- filtering becomes less effective and may result in more artifacts/distortion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ssp_spectorgram(f: WaveFile):\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14,4))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    ax = axes[0]\n",
    "    ax.set_title(f\"SPP spectrogram ({f.filename})\", fontdict={\"size\": 10})\n",
    "    ax.set_xlabel(\"time [s]\")\n",
    "    ax.set_ylabel(\"frequency [Hz]\")\n",
    "    im = ax.imshow(\n",
    "        f.m_spp,\n",
    "        extent=[f.v_time[0], f.v_time[-1], f.v_freq[0], f.v_freq[-1]],\n",
    "        origin=\"lower\",\n",
    "        aspect=\"auto\",\n",
    "    )\n",
    "    colorbar = fig.colorbar(im, orientation=\"vertical\")\n",
    "    colorbar.set_label(\"dB\")\n",
    "\n",
    "    ax = axes[1]\n",
    "    ax.set_title(f\"Noise PSD spectogram ({f.filename})\", fontdict={\"size\": 10})\n",
    "    ax.set_xlabel(\"time [s]\")\n",
    "    ax.set_ylabel(\"frequency [Hz]\")\n",
    "    im = ax.imshow(\n",
    "        10*np.log10(f.m_noise_psd),\n",
    "        extent=[f.v_time[0], f.v_time[-1], f.v_freq[0], f.v_freq[-1]],\n",
    "        origin=\"lower\",\n",
    "        aspect=\"auto\",\n",
    "    )\n",
    "    colorbar = fig.colorbar(im, orientation=\"vertical\")\n",
    "    colorbar.set_label(\"dB\")\n",
    "\n",
    "plot_ssp_spectorgram(speech_babble)\n",
    "plot_ssp_spectorgram(speech_white)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot input signal spectograms\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14,4))\n",
    "axes = axes.flatten()\n",
    "\n",
    "ax = axes[0]\n",
    "ax.set_title(f\"Input signal spectrogram ({speech_babble.filename})\", fontdict={\"size\": 10})\n",
    "ax.set_xlabel(\"time [s]\")\n",
    "ax.set_ylabel(\"frequency [Hz]\")\n",
    "im = ax.imshow(\n",
    "    10*np.log10(np.square(np.abs(speech_babble.m_stft))),\n",
    "    extent=[speech_babble.v_time[0], speech_babble.v_time[-1], speech_babble.v_freq[0], speech_babble.v_freq[-1]],\n",
    "    origin=\"lower\",\n",
    "    aspect=\"auto\",\n",
    ")\n",
    "colorbar = fig.colorbar(im, orientation=\"vertical\")\n",
    "colorbar.set_label(\"dB\")\n",
    "\n",
    "ax = axes[1]\n",
    "ax.set_title(f\"Input signal spectrogram ({speech_white.filename})\", fontdict={\"size\": 10})\n",
    "ax.set_xlabel(\"time [s]\")\n",
    "ax.set_ylabel(\"frequency [Hz]\")\n",
    "im = ax.imshow(\n",
    "    10*np.log10(np.abs(speech_white.m_stft)),\n",
    "    extent=[speech_white.v_time[0], speech_white.v_time[-1], speech_white.v_freq[0], speech_white.v_freq[-1]],\n",
    "    origin=\"lower\",\n",
    "    aspect=\"auto\",\n",
    ")\n",
    "colorbar = fig.colorbar(im, orientation=\"vertical\")\n",
    "colorbar.set_label(\"dB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2 A priori SNR estimation and Wiener Filtering**\n",
    "\n",
    "In this part of the exercise, we will obtain an estimate of the a priori SNR $\\hat\\xi$ from our noise power estimate\n",
    "$\\hat\\sigma^2_n[k, \\ell]$ using the decision-directed approach. It is given by (5):\n",
    "$$\n",
    "\\hat\\xi[k,\\ell] = \\alpha\\frac{|\\hat S[k,\\ell-1]|^2}{\\hat\\sigma^2_n[k,\\ell]} + (1-\\alpha) max(\\frac{|Y[k,\\ell]|^2}{\\hat\\sigma^2_n[k,\\ell]}-1, 0)\n",
    "$$\n",
    "\n",
    "Here, $\\hat S$ denotes the estimate of the clean speech spectrum $S$. The gain function $G$ of the Wiener-Filter is (6):\n",
    "$$\n",
    "G[k,\\ell] = max(\\frac{\\hat\\xi[k,\\ell]}{1+\\hat\\xi[k,\\ell]}, G_{min})\n",
    "$$\n",
    "\n",
    "The enhanced spectra are finally obtained via (7):\n",
    "$$\n",
    "\\hat S[k,\\ell] = G[k,\\ell] Y[k,\\ell]\n",
    "$$\n",
    "\n",
    "Perform the steps in equations (5) – (7) for every frame of your input signal and store all enhanced speech spectra $\\hat S[k,\\ell]$ in a matrix, e.g. `m_enhanced_stft`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def a_priori_snr(f: WaveFile, alpha: float, g_min: float) -> np.ndarray:\n",
    "    num_frames = f.m_stft.shape[0]\n",
    "    num_bins = f.m_stft.shape[1]\n",
    "\n",
    "    xi = np.zeros((num_frames, num_bins), dtype=\"complex_\")\n",
    "    gain = np.zeros((num_frames, num_bins), dtype=\"complex_\")\n",
    "    m_enhanced_stft = np.zeros((num_frames, num_bins+1), dtype=\"complex_\")\n",
    "\n",
    "    # loop over frequency bins\n",
    "    for i in range(num_bins):\n",
    "        # a priori SNR estimate (5)\n",
    "        xi[:, i] = alpha * m_enhanced_stft[:, i] / f.m_noise_psd[:, i]  + (1 - alpha) * np.fmax(f.m_signal_psd[:, i] / f.m_noise_psd[:, i] - 1, 0)\n",
    "        \n",
    "        # gain function G of the Wiener-Filter (6)\n",
    "        gain[:, i] = np.maximum(xi[:, i] / (1 + xi[:, i]), g_min)\n",
    "\n",
    "        # enhanced speech spectra (7)\n",
    "        m_enhanced_stft[:, i+1] = f.m_stft[:, i] * gain[:, i]\n",
    "    \n",
    "    return m_enhanced_stft\n",
    "\n",
    "speech_babble.set_enhanced_stft(a_priori_snr(speech_babble, 0.5, 0))\n",
    "speech_white.set_enhanced_stft(a_priori_snr(speech_white, 0.5, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Questions*\n",
    "\n",
    "2.1 In equation (5), the issue from Section 1 occurs again. An initialization for $\\hat S[k, \\ell]$ is required for the first\n",
    "frame.\n",
    "\n",
    "a) What would be a reasonable choice in this case?\n",
    "- we are using the original first frame of the input signal\n",
    "\n",
    "2.2 Set $\\alpha = 0.5$ and $G_{min} = 0$. Plot the magnitude spectrogram of the noisy speech signal and the enhanced speech signal in dB and compare both. Make sure, that the color bar for both plots is the same. This can be achieved by manually setting the `vmin` and `vmax` parameters of `plt.imshow`.\n",
    "\n",
    "a) How does the clean spectrogram differ from the spectrogram of the noisy input signal?\n",
    "- areas with low speech probablity (i.e. high probability for noise) have been removed from the signal\n",
    "\n",
    "b) Can you see artifacts in the spectrogram of the enhanced signal?\n",
    "- yes, especially in the white noise signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_enhanced_spectogram(f: WaveFile):\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14,4))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    vmin = -100\n",
    "    vmax = 40\n",
    "    \n",
    "    ax = axes[0]\n",
    "    ax.set_title(f\"Noisy spectogram ({f.filename})\", fontdict={\"size\": 10})\n",
    "    ax.set_xlabel(\"time [s]\")\n",
    "    ax.set_ylabel(\"frequency [Hz]\")\n",
    "    im = ax.imshow(\n",
    "        10*np.log10(np.fmax(np.abs(f.m_stft)**2, 10**-15)),\n",
    "        extent=[f.v_time[0], f.v_time[-1], f.v_freq[0], f.v_freq[-1]],\n",
    "        origin=\"lower\",\n",
    "        aspect=\"auto\",\n",
    "        vmin=vmin, \n",
    "        vmax=vmax,\n",
    "    )\n",
    "    colorbar = fig.colorbar(im, orientation=\"vertical\")\n",
    "    colorbar.set_label(\"dB\")\n",
    "\n",
    "    ax = axes[1]\n",
    "    ax.set_title(f\"Enhanced spectogram ({f.filename})\", fontdict={\"size\": 10})\n",
    "    ax.set_xlabel(\"time [s]\")\n",
    "    ax.set_ylabel(\"frequency [Hz]\")\n",
    "    im = ax.imshow(\n",
    "        10*np.log10(np.fmax(np.abs(f.m_enhanced_stft[:, 2:]**2), 10**(-15))),\n",
    "        extent=[f.v_time[0], f.v_time[-1], f.v_freq[0], f.v_freq[-1]],\n",
    "        origin=\"lower\",\n",
    "        aspect=\"auto\",\n",
    "        vmin=vmin, \n",
    "        vmax=vmax,\n",
    "    )\n",
    "    colorbar = fig.colorbar(im, orientation=\"vertical\")\n",
    "    colorbar.set_label(\"dB\")\n",
    "\n",
    "plot_enhanced_spectogram(speech_babble)\n",
    "plot_enhanced_spectogram(speech_white)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3 Parameter tuning**\n",
    "\n",
    "Use the `compute_istft` function to synthesize the enhanced speech signal. For this, employ the same frame shift and FFT length as in Section 1. Further, use the $\\sqrt{Hann}$-window also for the synthesis.\n",
    "\n",
    "Listen to the noisy signal and the enhanced signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hann_window = np.sqrt(get_window(\"hann\", speech_babble.samples_per_frame, True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_min = 0\n",
    "alpha = 0.9\n",
    "\n",
    "speech_babble.set_enhanced_stft(a_priori_snr(speech_babble, 0, g_min))\n",
    "speech_babble_enhanced = compute_istft(speech_babble.m_enhanced_stft[:, 2:], speech_babble.sr, frame_shift, hann_window)\n",
    "\n",
    "# sd.play(speech_babble.y, speech_babble.sr, blocking=True)\n",
    "# sd.play(speech_babble.y, speech_babble.sr, blocking=True)\n",
    "sd.play(speech_babble_enhanced, speech_babble.sr, blocking=True)\n",
    "\n",
    "speech_babble.set_enhanced_stft(a_priori_snr(speech_babble, 0.9, g_min))\n",
    "speech_babble_enhanced = compute_istft(speech_babble.m_enhanced_stft[:, 2:], speech_babble.sr, frame_shift, hann_window)\n",
    "sd.play(speech_babble_enhanced, speech_babble.sr, blocking=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_min = 0\n",
    "alpha = 0.5\n",
    "\n",
    "speech_white.set_enhanced_stft(a_priori_snr(speech_white, alpha, g_min))\n",
    "speech_white_enhanced = compute_istft(speech_white.m_enhanced_stft[:, 2:], speech_white.sr, frame_shift, hann_window)\n",
    "\n",
    "sd.play(speech_white.y, speech_white.sr, blocking=True)\n",
    "sd.play(speech_white_enhanced, speech_white.sr, blocking=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "**3.1 Compare the noisy and the enhanced signal with each other.**\n",
    "\n",
    "Parameter values: $\\alpha = 0.5$ and $G_{min} = 0$\n",
    "\n",
    "a) How well is the background noise suppressed?\n",
    "- okay-ish for the babble noise\n",
    "- not well at all for the white noise\n",
    "\n",
    "b) Can you hear any distortions of the speech signal in the enhanced signal?\n",
    "- yes, primarily in the white noise signal\n",
    "\n",
    "c) What artifacts can you hear?\n",
    "- high-pitched noise\n",
    "\n",
    "**3.2 Vary $\\alpha$ between 0 and 1 and listen to the synthesized signals.**\n",
    "\n",
    "a) What differences can you perceive?\n",
    "- $\\alpha = 0$\n",
    "    - works well for babble noise\n",
    "    - fairly minimal enhancement but less artifacts for white noise\n",
    "- $\\alpha = 1$\n",
    "    - setting $\\alpha$ to 1 results in a 0 in equation (5) resulting an empty signal\n",
    "\n",
    "b) How do the artifacts, speech signal and noise suppression change?\n",
    "- the lower the value of $\\alpha$, the more 'agressive' the filtering leading to higher noise supression\n",
    "- the higher the value of $\\alpha$, the more distortion, especially in the white noise signal\n",
    "\n",
    "c) What is your favorite setting? Explain why.\n",
    "- The optimal value for $\\alpha$ depends on the type of noise\n",
    "- $\\alpha = 0.5$ seems like a sensible choice for both babble and white noise\n",
    "\n",
    "d) Are the differences you heard also visible in the spectrogram of the enhanced speech signal?\n",
    "- yes see below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_variable_alpha(f: WaveFile, g_min: float):\n",
    "    fig, axes = plt.subplots(4, 3, figsize=(18,14))\n",
    "    fig.suptitle(f\"{f.filename} (g_min = {g_min})\")\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    enhanced_stfts = []\n",
    "    def on_click(event):\n",
    "        if not event.inaxes: return\n",
    "        if str(event.button) != \"MouseButton.LEFT\": return\n",
    "        for i, ax in enumerate(axes): \n",
    "            if event.inaxes == ax: break\n",
    "        if len(enhanced_stfts) < i+1: return\n",
    "        sd.play(compute_istft(enhanced_stfts[i][:, 2:], f.sr, frame_shift, hann_window), f.sr)\n",
    "\n",
    "    fig.canvas.mpl_connect(\"button_press_event\", on_click)\n",
    "    \n",
    "    snr = 10\n",
    "    vmin = -100\n",
    "    vmax = 40\n",
    "\n",
    "    for i in range(len(axes)):\n",
    "        alpha = i / 10\n",
    "        if alpha > 1:\n",
    "            break\n",
    "\n",
    "        m_enhanced_stft = a_priori_snr(f, alpha, g_min)\n",
    "        enhanced_stfts.append(m_enhanced_stft)\n",
    "        ax = axes[i]\n",
    "        ax.set_title(f\"Enhanced spectogram (alpha = {alpha})\", fontdict={\"size\": 10})\n",
    " \n",
    "        im = ax.imshow(\n",
    "            10*np.log10(np.fmax(np.abs(m_enhanced_stft[:, 2:]**2), 10**-15)),\n",
    "            extent=[f.v_time[0], f.v_time[-1], f.v_freq[0], f.v_freq[-1]],\n",
    "            origin=\"lower\",\n",
    "            aspect=\"auto\",\n",
    "            vmin=vmin, \n",
    "            vmax=vmax,\n",
    "        )\n",
    "        colorbar = fig.colorbar(im, orientation=\"vertical\")\n",
    "        colorbar.set_label(\"dB\")\n",
    "    \n",
    "plot_variable_alpha(speech_babble, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_variable_alpha(speech_white, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.3 Try different values for $G_{min}$ which can be varied between $0$ and $1$, i. e. between $-\\infty$ dB and $0$ dB. Listen again to the synthesized signals.**\n",
    "\n",
    "a) What differences can be perceived now?\n",
    "\n",
    "b) How does this parameter affect the artifacts, speech signal and noise suppression and what would be your favorite setting this time? Again, explain why.\n",
    "\n",
    "c) How do the spectrograms change in this case?\n",
    "\n",
    "In your report, include some spectrograms that support your reasoning for your choice of the parameters $\\alpha$ and $G_{min}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_variable_g_min(f: WaveFile, alpha: float):\n",
    "    fig, axes = plt.subplots(4, 3, figsize=(18,14))\n",
    "    fig.suptitle(f\"{f.filename} (alpha = {alpha})\")\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    enhanced_stfts = []\n",
    "    def on_click(event):\n",
    "        if not event.inaxes: return\n",
    "        if str(event.button) != \"MouseButton.LEFT\": return\n",
    "        for i, ax in enumerate(axes): \n",
    "            if event.inaxes == ax: break\n",
    "        if len(enhanced_stfts) < i+1: return\n",
    "        sd.play(compute_istft(enhanced_stfts[i][:, 2:], f.sr, frame_shift, hann_window), f.sr)\n",
    "\n",
    "    fig.canvas.mpl_connect(\"button_press_event\", on_click)\n",
    "    \n",
    "    snr = 10\n",
    "    vmin = -100\n",
    "    vmax = 40\n",
    "\n",
    "    for i in range(len(axes)):\n",
    "        g_min = i / 10\n",
    "        if g_min > 1:\n",
    "            break\n",
    "\n",
    "        m_enhanced_stft = a_priori_snr(f, alpha, g_min)\n",
    "        enhanced_stfts.append(m_enhanced_stft)\n",
    "        ax = axes[i]\n",
    "        ax.set_title(f\"Enhanced spectogram (g_min = {g_min})\", fontdict={\"size\": 10})\n",
    " \n",
    "        im = ax.imshow(\n",
    "            10*np.log10(np.fmax(np.abs(m_enhanced_stft[:, 2:]**2), 10**-15)),\n",
    "            extent=[f.v_time[0], f.v_time[-1], f.v_freq[0], f.v_freq[-1]],\n",
    "            origin=\"lower\",\n",
    "            aspect=\"auto\",\n",
    "            vmin=vmin, \n",
    "            vmax=vmax,\n",
    "        )\n",
    "        colorbar = fig.colorbar(im, orientation=\"vertical\")\n",
    "        colorbar.set_label(\"dB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_variable_g_min(speech_babble, 0.5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
