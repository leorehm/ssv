{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sounddevice as sd\n",
    "from scipy.signal import get_window, periodogram\n",
    "\n",
    "from util import my_windowing, compute_stft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the file Exercise6.zip from Moodle which contains two speech signals which are corrupted once by white noise and once by babble noise. For both files the signal-to-noise ratio (SNR) is 10 dB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WaveFile:\n",
    "    filename: str\n",
    "    y: np.ndarray\n",
    "    sr: int\n",
    "    num_samples: int\n",
    "\n",
    "    def __init__(self, filename: str) -> None:\n",
    "        self.filename = filename\n",
    "        self.y, self.sr = librosa.load(filename, sr=None)\n",
    "\n",
    "    def play(self):\n",
    "         sd.play(self.y, self.sr)\n",
    "\n",
    "    def _split_frames(self, frame_length, frame_shift):\n",
    "        self.m_frames, self.v_frame_times = my_windowing(\n",
    "            self.y,\n",
    "            self.sr,\n",
    "            frame_length,\n",
    "            frame_shift\n",
    "        )\n",
    "        self.num_frames = self.m_frames.shape[0]\n",
    "        self.samples_per_frame = self.m_frames.shape[1]\n",
    "    \n",
    "    def compute_stft(self, frame_length: int, frame_shift: int, window: str):\n",
    "        self._split_frames(frame_length, frame_shift)\n",
    "        \n",
    "        v_analysis_window = np.sqrt(get_window(window=window, Nx=self.samples_per_frame, fftbins=True))\n",
    "\n",
    "        # create frequency vector\n",
    "        v_freq = self.sr * (np.arange(self.samples_per_frame / 2 + 1) / self.samples_per_frame)\n",
    "\n",
    "        # calculate DFT\n",
    "        _m_frames = self.m_frames\n",
    "        _m_frames *= v_analysis_window\n",
    "        dft = np.fft.fft(_m_frames, axis=1)\n",
    "\n",
    "        # drop upper half of the spectrum while keeping the nyquist frequency in the middle\n",
    "        num_freq_bins = dft.shape[1]\n",
    "        if num_freq_bins % 2 == 0: # even \n",
    "            dft = dft[:, : num_freq_bins // 2 + 1]\n",
    "        else: # odd\n",
    "            dft = dft[:, : (num_freq_bins + 1) // 2]\n",
    "        m_stft = dft\n",
    "\n",
    "        np.testing.assert_array_almost_equal(m_stft, np.fft.rfft(_m_frames, axis=1))\n",
    "        \n",
    "        self.m_stft = m_stft\n",
    "        self.v_stft_freq = v_freq\n",
    "\n",
    "speech_babble = WaveFile(\"SpeechBabble.wav\")\n",
    "speech_white = WaveFile(\"SpeechWhite.wav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1 Noise Power Estimation**\n",
    "\n",
    "In this part of the exercise, the noise power is estimated based on the speech presence probability. First, use\n",
    "your functions from the second exercise to create the STFT of the noisy input signals. The frame length should\n",
    "be 32 ms and the frame shift 16 ms. Use a √Hann-window as analysis window. From the STFT, compute the\n",
    "periodograms and determine the noise PSD by performing the following steps for each frame.\n",
    "\n",
    "1. Compute the posterior probability of speech presence\n",
    "2. Avoid stagnations by smoo\n",
    "\n",
    "- Signal-to-noise ratio (SNR): $\\theta = 15 \\ dB$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.13763318, 0.95710912, 0.98311113, ..., 0.13234194, 0.06142717,\n",
       "        0.14900829],\n",
       "       [0.13763318, 0.18495219, 0.10463997, ..., 0.09268764, 0.05943763,\n",
       "        0.05883979],\n",
       "       [0.13763318, 0.09017056, 0.07384439, ..., 0.06924541, 0.06388317,\n",
       "        0.06609878],\n",
       "       ...,\n",
       "       [0.13763318, 0.08264102, 0.06545129, ..., 0.06745468, 0.06344599,\n",
       "        0.06240184],\n",
       "       [0.13763318, 0.09024271, 0.07333933, ..., 0.05896852, 0.05937047,\n",
       "        0.05936963],\n",
       "       [0.13763318, 0.13111914, 0.11489084, ..., 0.06029029, 0.05991479,\n",
       "        0.05978692]])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frame_length = 32\n",
    "frame_shift = 16\n",
    "\n",
    "speech_babble.compute_stft(frame_length, frame_shift, \"hann\")\n",
    "speech_white.compute_stft(frame_length, frame_shift, \"hann\")\n",
    "\n",
    "# TODO: warum gehen wir durch die frequency bins und nicht frames? \n",
    "#       was verändert sich wenn wir stattdessen über die frames loopen?\n",
    "def compute_ssp(signal: WaveFile, snr: int):\n",
    "    m_frames_psd = np.abs(signal.m_stft)**2\n",
    "    num_frames = m_frames_psd.shape[0]\n",
    "    num_bins = m_frames_psd.shape[1]\n",
    "    \n",
    "    m_ssp = np.zeros((num_frames, num_bins))\n",
    "    m_noise_psd = np.zeros((num_frames, num_bins + 1))\n",
    "    Q = np.zeros((num_frames, num_bins + 1))\n",
    "\n",
    "    # init noise periodogram\n",
    "    m_noise_psd[:, 0] = np.abs(signal.m_stft[:, 0])**2\n",
    "\n",
    "    # loop over frequency bins\n",
    "    for i in range(num_bins):\n",
    "        m_ssp[:, i] = (1 + (1 + snr) * np.exp(-(m_frames_psd[:, i] / m_noise_psd[:, i]) * (snr / (1 + snr))))**(-1)\n",
    "\n",
    "        Q[:, i+1] = 0.9 * Q[:, i] + 0.1 * m_ssp[:, i]\n",
    "\n",
    "        for j in range(num_frames):\n",
    "            if Q[j, i+1] > 0.99:\n",
    "                m_ssp[j, i] = min(m_ssp[j, i], 0.99)\n",
    "\n",
    "        ppp = m_ssp[:, i] * m_noise_psd[:, i] + (1 - m_ssp[:, i]) * m_noise_psd[:, i]\n",
    "        # m_noise_psd[:, i+1] = np.abs(signal.m_stft[:, i])**2\n",
    "        m_noise_psd[:, i+1] = 0.8 * m_noise_psd[:, i] + 0.2 * ppp\n",
    "\n",
    "    return m_ssp\n",
    "\n",
    "compute_ssp(speech_white, 15)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions**\n",
    "\n",
    "1.1 In equation (1), (3) and (4) the noise power estimate from the previous frame, ˆσ2n[k,` −1], is required.\n",
    "However, there is none available for the first frame ` = 0. The same problem occurs for the smoothed speech\n",
    "presence probability Q[k,`] in (2).\n",
    "\n",
    "- What would be appropriate initializations for the first noise estimate ˆσ2n[k,−1] and the smoothed\n",
    "posterior probability Q[k,−1]? Explain briefly why you chose your initialization method.\n",
    "\n",
    "1.2 Plot the speech presence probability P(H1|Y [k,`]) using plt.imshow.\n",
    "\n",
    "a) Which values do you obtain for time-frequency points where speech is present?\n",
    "\n",
    "b) What values do you get for time-frequency points where only noise is present?\n",
    "\n",
    "c) If you compare the speech presence probability with the spectrogram of your input signal, can you see\n",
    "similarities?\n",
    "\n",
    "1.3 Plot the estimated noise PSD as a spectrogram using plt.imshow.\n",
    "\n",
    "a) How well is the background noise estimated?\n",
    "\n",
    "b) Can you observe errors (e. g. components that do not belong to the background noise)?\n",
    "\n",
    "c) What would be the consequence of such errors?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2 A priori SNR estimation and Wiener Filtering**\n",
    "\n",
    "Perform the steps in equations (5) – (7) for every frame of your input signal and store all enhanced speech\n",
    "spectra ˆS[k,`] in a matrix, e. g. m_enhanced_stft. (Look at sheet for the steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Questions*\n",
    "\n",
    "2.1 In equation (5), the issue from Section 1 occurs again. An initialization for ˆS[k,`] is required for the first\n",
    "frame.\n",
    "\n",
    "*What would be a reasonable choice in this case?*\n",
    "\n",
    "2.2 Set α = 0.5 and Gmin = 0. Plot the magnitude spectrogram of the noisy speech signal and the enhanced\n",
    "speech signal in dB and compare both. Make sure, that the color bar for both plots is the same. This can\n",
    "be achieved by manually setting the vmin and vmax parameters of plt.imshow.\n",
    "\n",
    "a) How does the clean spectrogram differ from the spectrogram of the noisy input signal?\n",
    "\n",
    "b) Can you see artifacts in the spectrogram of the enhanced signal?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3 Parameter tuning**\n",
    "\n",
    "Use the compute_istft function to synthesize the enhanced speech signal. For this, employ the same frame shift\n",
    "and FFT length as in Section 1. Further, use the √Hann-window also for the synthesis.\n",
    "\n",
    "Listen to the noisy signal and the enhanced signal. For this you could use the play function from sounddevice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Questions*\n",
    "\n",
    "3.1 Compare the noisy and the enhanced signal with each other.\n",
    "\n",
    "a) How well is the background noise suppressed?\n",
    "\n",
    "b) Can you hear any distortions of the speech signal in the enhanced signal?\n",
    "\n",
    "c) What artifacts can you hear?\n",
    "\n",
    "3.2 Vary α between 0 and 1 and listen to the synthesized signals.\n",
    "\n",
    "a) What differences can you perceive?\n",
    "\n",
    "b) How do the artifacts, speech signal and noise suppression change?\n",
    "\n",
    "c) What is your favorite setting? Explain why.\n",
    "\n",
    "d) Are the differences you heard also visible in the spectrogram of the enhanced speech signal?\n",
    "\n",
    "3.3 Try different values for Gmin which can be varied between 0 and 1, i. e. between −∞ dB and 0 dB. Listen\n",
    "again to the synthesized signals.\n",
    "\n",
    "a) What differences can be perceived now?\n",
    "\n",
    "b) How does this parameter affect the artifacts, speech signal and noise suppression and what would be\n",
    "your favorite setting this time? Again, explain why.\n",
    "\n",
    "c) How do the spectrograms change in this case?\n",
    "\n",
    "In your report, include some spectrograms that support your reasoning for your choice of the parameters α and\n",
    "Gmin.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
